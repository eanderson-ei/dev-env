{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MI2 Survey Data Indicator Scripting\n",
    "This notebook contains scripts for munging data from the MI2 survey and calculating indicator values. Where applicable, indicator values are also caluclated per respondent.\n",
    "\n",
    "**Contents**\n",
    "* [Data Munging Script](#Data-Munging-Script)\n",
    "* [Indicators](#Indicators)\n",
    " - [Indicator 1.1a](#Indicator-1.1a)\n",
    " - [Indicator 1.1b](#Indicator-1.1b)\n",
    " - [Indicator 1.1c](#Indicator-1.1c)\n",
    " - [Indicator 1.1](#Indicator-1.1)\n",
    " - [Indicator 4.1](#Indicator-4.1)\n",
    " - [Indicator 5.1](#Indicator-5.1)\n",
    " - [Indicator 6.1](#Indicator-6.1)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Munging Script\n",
    "This script codes survey questions and concatenates identically coded questions into a single column. Where multiple responses are allowed, responses will be separated by a comma. A cleaned spreadsheet with human readable responses is saved out as `clean_df.csv` and a coded spreadsheet is saved as `coded_df.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "raw_coded = pd.read_excel(r'C:\\Users\\Erik\\OneDrive - Environmental Incentives\\Downloads\\mi2-data.xlsx', \n",
    "                          'RAW_CODED', \n",
    "                          header=None, # can't read duped headers\n",
    "                          engine='openpyxl', \n",
    "                          date_parser=[3,4],\n",
    "                          na_values='.')\n",
    "codes = pd.read_excel(r'C:\\Users\\Erik\\OneDrive - Environmental Incentives\\Downloads\\mi2-data-xwalk.xlsx', \n",
    "                      'xwalk', \n",
    "                      engine='openpyxl',\n",
    "                     index_col='original')\n",
    "\n",
    "recodes = pd.read_excel(r'C:\\Users\\Erik\\OneDrive - Environmental Incentives\\Downloads\\mi2-data-xwalk.xlsx', \n",
    "                      'recodes', \n",
    "                      engine='openpyxl',\n",
    "                      index_col='column')\n",
    "\n",
    "# rename headers (can't read in duplicate headers, allowed for rename)\n",
    "raw_coded.columns = codes['coded']\n",
    "raw_coded = raw_coded.drop(0, axis=0)\n",
    "raw_coded = raw_coded.dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate identical questions and drop response row\n",
    "columns = raw_coded.columns\n",
    "column_filters = [columns.get_loc(c) for c in columns]  # -> int if unique else boolean array \n",
    "\n",
    "series = []\n",
    "already_seen = set()\n",
    "for i in column_filters:\n",
    "    if type(i)==int:\n",
    "        series.append(raw_coded.iloc[:,i])\n",
    "    elif type(i)==np.ndarray:\n",
    "        s = raw_coded.iloc[:,i].copy()\n",
    "        name = s.columns[0]\n",
    "        if name not in already_seen:\n",
    "            s = s.apply(lambda x: ', '.join(x.dropna().astype(str)), axis=1)\n",
    "            s.name = name\n",
    "            series.append(s)\n",
    "            already_seen.add(name)\n",
    "clean_df = pd.concat(series, axis=1)\n",
    "\n",
    "# convert all empty cells to nan\n",
    "clean_df = clean_df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "\n",
    "# convert responses to yes/no when multiple values allowed\n",
    "multi_cols = ['BD3-1str', 'BD3-1prj', 'BD3-1ad', 'BD3-1asu', 'BD3-1imp', 'BD3-1evl']\n",
    "for col in multi_cols:\n",
    "    clean_df[col] = clean_df[col].fillna('No')\n",
    "    clean_df[col] = clean_df[col].apply(lambda x: 'Yes' if x != 'No' else 'No')\n",
    "\n",
    "clean_df.to_csv('clean_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code categorical columns\n",
    "coded_df = clean_df.copy()\n",
    "\n",
    "# specify dictionaries for encoding\n",
    "yes_no = {\n",
    "    'Yes': 1,\n",
    "    'No': 0,\n",
    "    'Not sure': 2\n",
    "}\n",
    "\n",
    "roles = {\n",
    "    'I was part of a team responsible for managing a biodiversity program or programming biodiversity funds (Biodiversity Practitioner and/or A/COR)': 1,\n",
    "    'I was part of a team responsible for managing a biodiversity program or programming biodiversity funds (Biodiversity Practitioner or A/COR)': 1,\n",
    "    'I advised and/or provided technical assistance to those managing biodiversity programs or programming biodiversity funds (Biodiversity Advisor)': 2,\n",
    "    'I engaged with biodiversity programming through facilitating strategic planning, procurement, and/or M&E and reporting (Program Officer)': 3,\n",
    "    'I facilitated strategic planning, procurement, and/or M&E and reporting (Program Officer)': 3,\n",
    "    'I worked in a different sector at USAID and supported one or more programs that included cross-sectoral integration with biodiversity (Partner in Integration)': 4,\n",
    "    'I work in a different sector at USAID and supported cross-sectoral integration with biodiversity in a project or activity (Partner in Integration)': 4,\n",
    "    'Other (please specify)': 5\n",
    "}\n",
    "\n",
    "roles_short = {\n",
    "    'Strategy (CDCS)': 1,\n",
    "    'Project design (PAD)': 2,\n",
    "    'Activity design and/or procurement': 3,\n",
    "    'Activity start up': 4,\n",
    "    'Activity implementation': 5,\n",
    "    'Evaluation': 6,\n",
    "    'Other (please specify)': 7\n",
    "}\n",
    "\n",
    "op_units = {\n",
    "    'Office of Forestry and Biodiversity': 1,\n",
    "    'Other Washington Office or Operating Unit': 2,\n",
    "    'Mission (please specify)': 3\n",
    "}\n",
    "\n",
    "likert = {\n",
    "    'Strongly disagree': 1,\n",
    "    'Disagree': 2,\n",
    "    'Neither agree nor disagree':3,\n",
    "    'Agree': 4,\n",
    "    'Strongly Agree': 5\n",
    "}\n",
    "\n",
    "importance = {\n",
    "    'Not important': 1,\n",
    "    'Slightly important': 2,\n",
    "    'Moderately important': 3,\n",
    "    'Important': 4,\n",
    "    'Very important': 5,\n",
    "    \"Don't know\": 6\n",
    "}\n",
    "\n",
    "familiar = {\n",
    "    'Not at all familiar': 1,\n",
    "    'Somewhat familiar': 2,\n",
    "    'Very familiar': 3\n",
    "}\n",
    "\n",
    "activities = {\n",
    "    'strategy': 1,\n",
    "    'project design': 2,\n",
    "    'activity design': 3,\n",
    "    'activity start up': 4,\n",
    "    'activity implementation': 5,\n",
    "    'evaluation': 6,\n",
    "    'other': 7\n",
    "}\n",
    "\n",
    "effects = {\n",
    "    'large positive effect': 1,\n",
    "    'moderate positive effect': 2,\n",
    "    'little effect': 3,\n",
    "    'negative effect': 4,\n",
    "    \"don't know\": 5\n",
    "}\n",
    "\n",
    "recode_dict = {\n",
    "    'yes_no': yes_no,\n",
    "    'roles': roles,\n",
    "    'roles_short': roles_short,\n",
    "    'op_units': op_units,\n",
    "    'likert': likert,\n",
    "    'importance': importance,\n",
    "    'familiar': familiar,\n",
    "    'activities': activities,\n",
    "    'effects': effects\n",
    "}\n",
    "\n",
    "# code\n",
    "for col in recodes.index:\n",
    "    coded_df[col] = coded_df[col].map(recode_dict.get(recodes.loc[col][0]))\n",
    "\n",
    "# code MI6.1-3 based on whether the text contains the key\n",
    "def replace_if_contains(x):\n",
    "    for key in effects.keys():\n",
    "        if key in str(x).lower():\n",
    "            return effects[key]\n",
    "\n",
    "coded_df['MI6.x'] = coded_df['MI6.x'].apply(replace_if_contains)\n",
    "    \n",
    "# save to csv\n",
    "coded_df.to_csv('coded_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicators\n",
    "Indicator values are calculated per respondent where necessary and concatenated to `clean_df.csv`. Results are saved out after all indicators are calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indicator 1.1a\n",
    "**Shared understanding of the value of AM for biodiversity programming**\t\t\t\t\t\n",
    "This sub-indicator is calculated as the percentage of respondents that agrees or strongly agrees that AM improves biodiversity outcomes converted to a 5-point scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['i1_1a_values'] = clean_df['AM6-3']\n",
    "clean_df['i1_1a'] = clean_df['i1_1a_values'].isin(['Strongly Agree', 'Agree'])\n",
    "filt = clean_df['i1_1a_values'].isna()\n",
    "clean_df['i1_1a'] = clean_df['i1_1a'].mask(filt, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'AM improves biodiversity outcomes'}, ylabel='i1_1a_values'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df['i1_1a_values'].value_counts().plot(kind='pie', autopct='%1.1f%%', title='AM improves biodiversity outcomes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 62,  82.26%\n"
     ]
    }
   ],
   "source": [
    "n = clean_df['i1_1a'].count()\n",
    "i = clean_df['i1_1a'].mean()\n",
    "print(f'n: {n}, {i: .2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indicator 1.1b\n",
    "**Shared understanding of the tools and practices recommended for AM in biodiversity programming converted to a 5-point scale.**\t\t\t\t\t\n",
    "This sub-indicator is calculated as the percentage of respondents that agrees or strongly agrees that the use of the four AM practices identified in the survey are important or very important in biodiverisity programming.\t\t\t\t\t\n",
    "*Note: if the respondent does not respond to any of the 4 prompts, they are excluded.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_count = clean_df[['PR5-5', 'PR6-5', 'PR7-4', 'PR8-5']].apply(pd.Series.value_counts, axis=1)\n",
    "clean_df['i1_1b_values'] = response_count[['Important', 'Very important']].sum(axis=1)\n",
    "clean_df['i1_1b'] = clean_df['i1_1b_values'] == 4\n",
    "# overwrite with na if any responses are missing\n",
    "filt = clean_df[['PR5-5', 'PR6-5', 'PR7-4', 'PR8-5']].isna().any(axis=1)\n",
    "clean_df['i1_1b'] = clean_df['i1_1b'].mask(filt, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Number of responses \"Important\" or \"Very important\"'}, ylabel='i1_1b_values'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df['i1_1b_values'].value_counts().plot(kind='pie', autopct='%1.1f%%', title='Number of responses \"Important\" or \"Very important\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 55,  83.64%\n"
     ]
    }
   ],
   "source": [
    "n = clean_df['i1_1b'].count()\n",
    "i = clean_df['i1_1b'].mean()\n",
    "print(f'n: {n}, {i: .2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicator 1.1c\n",
    "**Commitment to resourcing AM in biodiveristy programming**\t\t\t\t\n",
    "This sub-indicator is calculated as the percentage of respondents that agrees or strongly agrees that environment teams have access to the resources needed to adaptively manage their programs converted to a 5-point scale.\t\t\t\t\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['i1_1c_values'] = clean_df['AM_RES']\n",
    "clean_df['i1_1c'] = clean_df['i1_1c_values'].isin(['Strongly Agree', 'Agree'])\n",
    "# overwrite null values\n",
    "filt = clean_df['i1_1c_values'].isna()\n",
    "clean_df['i1_1c'] = clean_df['i1_1c'].mask(filt, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'environment teams have access to the resources needed to adaptively manage'}, ylabel='i1_1c_values'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df['i1_1c_values'].value_counts().plot(kind='pie', autopct='%1.1f%%', title='environment teams have access to the resources needed to adaptively manage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 59,  59.32%\n"
     ]
    }
   ],
   "source": [
    "n = clean_df['i1_1c'].count()\n",
    "i = clean_df['i1_1c'].mean()\n",
    "print(f'n: {n}, {i: .2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicator 1.1\n",
    "**Sector-wide shared understanding index**\t\t\t\t\n",
    "This is a composite indicator calculated as the sum of three sub-indicators associated with a shared understanding of AM (range 3-15).\t\t\t\t\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indicator 1.1 = 13\n"
     ]
    }
   ],
   "source": [
    "bins = np.array([0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "labels = [1, 2, 3, 4, 5]\n",
    "means_1_1 = clean_df[['i1_1a', 'i1_1b', 'i1_1c']].mean()\n",
    "scores_1_1 = pd.cut(means_1_1, bins=bins, labels=labels, right=False)  # right exclusive\n",
    "i1_1 = scores_1_1.astype(int).sum()\n",
    "print(f'Indicator 1.1 = {i1_1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicator 4.1\n",
    "**Percentage of respondents reporting that enabling conditions for EBP are in place**\t\t\t\t\n",
    "This indicator is calculated as the percentage of respondents assigning scores of 4 or 5 to both EBP enabling conditions identified in the survey. The sample only includes respondents who assessed both EBP enabling conditions.\t\t\t\t\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "agree_list = ['Agree', 'Strongly Agree']\n",
    "filt = (clean_df['EV_KSA'].isin(agree_list))\\\n",
    "    & (clean_df['EV_RES'].isin(agree_list))\n",
    "clean_df['i4_1'] = filt\n",
    "\n",
    "# overwrite with na if any responses are missing\n",
    "filt = (clean_df['EV_KSA'].isna()) \\\n",
    "    & (clean_df['EV_RES'].isna())\n",
    "clean_df['i4_1'] = clean_df['i4_1'].mask(filt, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'1.0: Assigned 4 or 5 to both conditions'}, ylabel='i4_1'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df['i4_1'].value_counts().plot(kind='pie', autopct='%1.1f%%', title='1.0: Assigned 4 or 5 to both conditions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 58,  39.66%\n"
     ]
    }
   ],
   "source": [
    "n = clean_df['i4_1'].count()\n",
    "i = clean_df['i4_1'].mean()\n",
    "print(f'n: {n}, {i: .2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicator 5.1\n",
    "**Percentage of respondents reporting broad uptake of AM practices in biodiversity programming**\t\t\t\n",
    "This indicator is calculated as the percentage of respondents reporting a high level of use across all four practices identified in the survey. The sample only includes respondents who assessed all four practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_count = clean_df[['PR1-5', 'PR2-4', 'PR3-4', 'PR4-3']].apply(pd.Series.value_counts, axis=1)\n",
    "clean_df['i5_1_values'] = response_count[['Agree', 'Strongly Agree']].sum(axis=1)\n",
    "clean_df['i5_1'] = clean_df['i5_1_values'] == 4\n",
    "\n",
    "# overwrite with na if any responses are missing\n",
    "filt = clean_df[['PR1-5', 'PR2-4', 'PR3-4', 'PR4-3']].isna().any(axis=1)\n",
    "clean_df['i5_1'] = clean_df['i5_1'].mask(filt, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Number of responses \"Agree\" or \"Strongly Agree\"'}, ylabel='i5_1_values'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df['i5_1_values'].value_counts().plot(kind='pie', autopct='%1.1f%%', title='Number of responses \"Agree\" or \"Strongly Agree\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 55,  56.36%\n"
     ]
    }
   ],
   "source": [
    "n = clean_df['i5_1'].count()\n",
    "i = clean_df['i5_1'].mean()\n",
    "print(f'n: {n}, {i: .2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicator 6.1\n",
    "**Percentage of respondents reporting that application of MI/MI2 practices had a positive effect on Program Cycle outputs**\t\t\t\t\n",
    "This indicator is calculated as the percentage of respondents reporting that MI/MI2 practices had a moderate or strong poistive effect on the Program Cycle output for which they received techncial assistance. The sample excludes respondents who received TA but reported that they did not know the effect of MI/MI2 practices on the Program Cycle output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['i6_1_values'] = clean_df['MI6.x']\n",
    "clean_df['i6_1'] = clean_df['i6_1_values'].str.contains('positive').astype(float)\n",
    "# mask instances of 'I don't know' note unicode character for apostrophe\n",
    "mask = clean_df['i6_1_values']=='I donâ€™t know'\n",
    "clean_df['i6_1'] = clean_df['i6_1'].mask(mask, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'1.0: MI2 had a positive effect'}, ylabel='i6_1'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df['i6_1'].value_counts().plot(kind='pie', autopct='%1.1f%%', title='1.0: MI2 had a positive effect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 40,  95.00%\n"
     ]
    }
   ],
   "source": [
    "n = clean_df['i6_1'].count()\n",
    "i = clean_df['i6_1'].mean()\n",
    "print(f'n: {n}, {i: .2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.to_csv('clean_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "coded_df = coded_df.merge(clean_df[['i1_1a', 'i1_1b',\n",
    "       'i1_1c', 'i4_1', 'i5_1', 'i6_1']], \n",
    "               left_index=True, \n",
    "               right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "coded_df.to_csv('coded_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
